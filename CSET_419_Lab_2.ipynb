{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHYXr5rDdVjjAdYMDMqxHp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CPT-Dawn/College-Lab-Work-CSET419-Introduction-to-Generative-Artificial-Intelligence/blob/main/CSET_419_Lab_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxuO3djjxkfZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "\n",
        "# ==========================================\n",
        "# STEP 1: INPUT PARAMETERS [cite: 14-21]\n",
        "# ==========================================\n",
        "def get_user_inputs():\n",
        "    print(\"--- GAN Configuration ---\")\n",
        "    # We use defaults for smoother execution, but allow overrides\n",
        "    choice = input(\"Dataset (mnist/fashion) [default: mnist]: \").strip().lower() or 'mnist'\n",
        "    epochs = int(input(\"Epochs [default: 50]: \").strip() or 50)\n",
        "    batch_size = int(input(\"Batch Size [default: 128]: \").strip() or 128)\n",
        "    noise_dim = int(input(\"Noise Dimension [default: 100]: \").strip() or 100)\n",
        "    # Learning rate is usually fixed in code for optimizers, but we can set it here\n",
        "    lr = 0.0002\n",
        "    save_interval = int(input(\"Save Interval (epochs) [default: 5]: \").strip() or 5)\n",
        "\n",
        "    return choice, epochs, batch_size, noise_dim, lr, save_interval\n",
        "\n",
        "# Get configuration\n",
        "DATASET_CHOICE, EPOCHS, BATCH_SIZE, NOISE_DIM, LR, SAVE_INTERVAL = get_user_inputs()\n",
        "\n",
        "# ==========================================\n",
        "# STEP 2: DATASET LOADING & PREPROCESSING [cite: 9-13, 30]\n",
        "# ==========================================\n",
        "def load_data(choice):\n",
        "    if choice == 'fashion':\n",
        "        (x_train, y_train), (_, _) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "    else:\n",
        "        (x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "    # Normalize images to [-1, 1] as required for GANs with Tanh output\n",
        "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
        "    x_train = (x_train - 127.5) / 127.5\n",
        "\n",
        "    # Batch and shuffle the data\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(60000).batch(BATCH_SIZE)\n",
        "    return dataset, (x_train, y_train) # Return raw data for classifier training later\n",
        "\n",
        "train_dataset, (raw_x, raw_y) = load_data(DATASET_CHOICE)\n",
        "\n",
        "# ==========================================\n",
        "# STEP 3: BUILD GENERATOR\n",
        "# ==========================================\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    # Start with a dense layer that takes the noise vector\n",
        "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(NOISE_DIM,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # Reshape into a 3D volume (7x7x256)\n",
        "    model.add(layers.Reshape((7, 7, 256)))\n",
        "\n",
        "    # Upsample to 14x14\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # Upsample to 14x14\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # Output layer: Upsample to 28x28.\n",
        "    # Tanh activation is used to map pixel values to [-1, 1]\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "\n",
        "    return model\n",
        "\n",
        "generator = make_generator_model()\n",
        "\n",
        "# ==========================================\n",
        "# STEP 4: BUILD DISCRIMINATOR\n",
        "# ==========================================\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Convolutional layers to classify image\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    # Flatten and output a single score (Real vs Fake)\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model\n",
        "\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "# ==========================================\n",
        "# STEP 5: LOSS & OPTIMIZERS [cite: 32]\n",
        "# ==========================================\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    # Loss for real images (should be 1)\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    # Loss for fake images (should be 0)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    # Generator wants discriminator to think fake images are real (1)\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(LR)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(LR)\n",
        "\n",
        "# ==========================================\n",
        "# STEP 6: TRAINING LOOP [cite: 26, 36-38]\n",
        "# ==========================================\n",
        "# Create directories for outputs [cite: 40, 43]\n",
        "os.makedirs('generated_samples', exist_ok=True)\n",
        "os.makedirs('final_generated_images', exist_ok=True)\n",
        "\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        # Generate images\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        # Discriminator judges real and fake images\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        # Calculate losses\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    # Apply gradients\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    # Calculate simple accuracy for logging\n",
        "    # Real is predicted as real if output > 0, Fake as fake if output < 0\n",
        "    real_acc = tf.reduce_mean(tf.cast(real_output > 0, tf.float32))\n",
        "    fake_acc = tf.reduce_mean(tf.cast(fake_output < 0, tf.float32))\n",
        "    disc_acc = (real_acc + fake_acc) * 0.5\n",
        "\n",
        "    return gen_loss, disc_loss, disc_acc\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    g_loss_metric = 0\n",
        "    d_loss_metric = 0\n",
        "    d_acc_metric = 0\n",
        "    batches = 0\n",
        "\n",
        "    for image_batch in train_dataset:\n",
        "        g_loss, d_loss, d_acc = train_step(image_batch)\n",
        "        g_loss_metric += g_loss\n",
        "        d_loss_metric += d_loss\n",
        "        d_acc_metric += d_acc\n",
        "        batches += 1\n",
        "\n",
        "    # Average metrics for the epoch\n",
        "    g_loss_avg = g_loss_metric / batches\n",
        "    d_loss_avg = d_loss_metric / batches\n",
        "    d_acc_avg = d_acc_metric / batches\n",
        "\n",
        "    # OUTPUT 1: Training Logs [cite: 37-38]\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS} | D_loss: {d_loss_avg:.2f} | D_acc: {d_acc_avg*100:.2f}% | G_loss: {g_loss_avg:.2f}')\n",
        "\n",
        "    # OUTPUT 2: Periodic Saving [cite: 27, 41]\n",
        "    if (epoch + 1) % SAVE_INTERVAL == 0:\n",
        "        noise = tf.random.normal([25, NOISE_DIM])\n",
        "        generated_images = generator(noise, training=False)\n",
        "\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        for i in range(25):\n",
        "            plt.subplot(5, 5, i+1)\n",
        "            plt.imshow(generated_images[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "            plt.axis('off')\n",
        "        plt.savefig(f'generated_samples/epoch_{epoch+1:02d}.png')\n",
        "        plt.close()\n",
        "\n",
        "# ==========================================\n",
        "# STEP 7: FINAL GENERATION & EVALUATION [cite: 42-45]\n",
        "# ==========================================\n",
        "print(\"Training Complete. Generating Final Images...\")\n",
        "\n",
        "# Generate 100 images\n",
        "noise = tf.random.normal([100, NOISE_DIM])\n",
        "final_images = generator(noise, training=False)\n",
        "\n",
        "# Save the 100 images\n",
        "for i in range(100):\n",
        "    img = final_images[i, :, :, 0] * 127.5 + 127.5\n",
        "    tf.keras.utils.save_img(f'final_generated_images/image_{i}.png', np.expand_dims(img, axis=-1))\n",
        "\n",
        "# Helper: Create a simple \"Pre-trained\" classifier since we don't have an external file\n",
        "# In a real scenario, you would load_model('my_classifier.h5')\n",
        "print(\"Training a temporary classifier for evaluation (Transfer Learning simulation)...\")\n",
        "classifier = tf.keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28, 1)),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10)\n",
        "])\n",
        "classifier.compile(optimizer='adam',\n",
        "                   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                   metrics=['accuracy'])\n",
        "# Train quickly on the real data we loaded earlier\n",
        "classifier.fit(raw_x, raw_y, epochs=1, verbose=0)\n",
        "\n",
        "# OUTPUT 4: Predict Labels [cite: 44-45]\n",
        "predictions = classifier.predict(final_images)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "print(\"\\n--- Evaluation Report ---\")\n",
        "print(\"Label Distribution of Generated Images:\")\n",
        "unique, counts = np.unique(predicted_labels, return_counts=True)\n",
        "for u, c in zip(unique, counts):\n",
        "    print(f\"Class {u}: {c} images\")\n",
        "\n",
        "print(\"\\nLab completed successfully!\")"
      ]
    }
  ]
}